{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning models demand that categorical features are converted to a format they can comprehend via a widely used feature engineering technique called **one-hot encoding**. Machines aren't *that* smart.\n",
    "\n",
    "A common convention after one-hot encoding is to remove one of the one-hot encoded columns from each categorical feature. For example, the feature `sex` containing values of `male` and `female` are transformed into the columns `sex_male` and `sex_female`, each containing binary values. Because using either of these columns provides sufficient information to determine a person's sex, we can drop one of them.\n",
    "\n",
    "In this post, we dive deep into the circumstances where this convention is relevant, necessary, or even prudent.\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "# Table of contents\n",
    "1. [Preparing the data](#cell1)\n",
    "2. [Creating a linear regression model with ordinary least-squares](#cell2)\n",
    "3. [Making the normal equation usable again](#cell3)\n",
    "4. [Regularizing improves predictions and then some](#cell4)\n",
    "5. [Don't bother dropping columns when regularizing](#cell5)\n",
    "6. [Skip dropping columns when using iterative numerical methods](#cell6)\n",
    "7. [Maybe just stop dropping columns altogether](#cell7)\n",
    "8. [Conclusions](#cell8)\n",
    "\n",
    "<a id=\"cell1\"></a>\n",
    "# Preparing the data\n",
    "\n",
    "Let's generate a toy dataset with three variables; the third column serves as the target variable while the remaining are categorical features. Because we're working with a continuous target variable, we'll create a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>dog</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>cat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pear</td>\n",
       "      <td>fish</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orange</td>\n",
       "      <td>dog</td>\n",
       "      <td>-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>fish</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pear</td>\n",
       "      <td>cat</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>apple</td>\n",
       "      <td>fish</td>\n",
       "      <td>-69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     var1  var2  var3\n",
       "0   apple   dog    10\n",
       "1  banana   cat     4\n",
       "2    pear  fish    39\n",
       "3  orange   dog   -12\n",
       "4   apple  fish    21\n",
       "5    pear   cat    53\n",
       "6   apple  fish   -69"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create training set\n",
    "training_set = pd.DataFrame(\n",
    "    [\n",
    "        ['apple', 'dog', 10],\n",
    "        ['banana', 'cat', 4],\n",
    "        ['pear', 'fish', 39],\n",
    "        ['orange', 'dog', -12],\n",
    "        ['apple', 'fish', 21],\n",
    "        ['pear', 'cat', 53],\n",
    "        ['apple', 'fish', -69]\n",
    "    ],\n",
    "    columns=['var1', 'var2', 'var3']\n",
    ")\n",
    "\n",
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the pandas function `get_dummies` to perform one-hot encoding and generate the feature matrix $\\mathbf{X}$.\n",
    "\n",
    "Let's also add a bias term to $\\mathbf{X}$ as a new column so that any model we create isn't confined to passing through the origin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1_apple</th>\n",
       "      <th>var1_banana</th>\n",
       "      <th>var1_orange</th>\n",
       "      <th>var1_pear</th>\n",
       "      <th>var2_cat</th>\n",
       "      <th>var2_dog</th>\n",
       "      <th>var2_fish</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var1_apple  var1_banana  var1_orange  var1_pear  var2_cat  var2_dog  \\\n",
       "0           1            0            0          0         0         1   \n",
       "1           0            1            0          0         1         0   \n",
       "2           0            0            0          1         0         0   \n",
       "\n",
       "   var2_fish  bias  \n",
       "0          0   1.0  \n",
       "1          0   1.0  \n",
       "2          1   1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode categorical features\n",
    "X = pd.get_dummies(training_set[['var1', 'var2']])\n",
    "\n",
    "# Add bias column\n",
    "X['bias'] = np.ones(X.shape[0])\n",
    "\n",
    "# Display first three rows\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's identify the target variable $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target variable\n",
    "y = training_set['var3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cell2\"></a>\n",
    "# Creating a linear regression model with ordinary least-squares\n",
    "\n",
    "In a linear regression model, we express the target variable $\\mathbf{y}$ as a linear function of the features $\\mathbf{X}$ and some unknown set of parameters $\\vec{\\theta}$:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X}\\vec{\\theta}$$\n",
    "\n",
    "The simplest algorithm for finding this \"line of best fit\" is **ordinary least-squares (OLS)**; it identifies $\\vec\\theta$ that minimizes the sum of the squared residuals. Therefore, the objective function for OLS is\n",
    "\n",
    "$$J(\\vec{\\theta}) = {\\left\\lVert \\mathbf{y} - \\mathbf{X}\\vec{\\theta} \\right\\rVert_2}^2$$\n",
    "\n",
    "Next, we have to solve the partial differential equation $\\frac{\\partial J}{\\partial\\vec{\\theta}} = 0$, which conveniently has a closed-form solution called the **normal equation**: \n",
    "\n",
    "$$\\vec{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "Let's apply the normal equation to identify the parameters of the OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d1b033489f2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compute parameters of OLS model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mOLS_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Label parameters with feature names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOLS_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/phoenix/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/phoenix/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Compute parameters of OLS model\n",
    "OLS_theta = np.linalg.inv(X.T @ X) @ (X.T @ y)\n",
    "\n",
    "# Label parameters with feature names\n",
    "pd.Series(OLS_theta, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy got angry because we tried to invert a singular matrix. Specifically, $\\mathbf{X}^T\\mathbf{X}$ (the Gram matrix of $\\mathbf{X}$) was found to be **singular**, meaning it doesn't have an inverse. In fact, [the Gram matrix is invertible](https://math.stackexchange.com/questions/36580/gram-matrix-invertible-iff-set-of-vectors-linearly-independent) if and only if the columns of $\\mathbf{X}$ are linearly independent.\n",
    "\n",
    "Examining the columns of $\\mathbf{X}$, we see that\n",
    "\n",
    "`var1_apple` = 1 - (`var1_orange` + `var1_pear` + `var1_banana`)\n",
    "\n",
    "`var2_cat` = 1 - (`var2_dog` + `var2_fish`)\n",
    "\n",
    "For any categorical feature, each one-hot encoded column can be expressed as a linear combination of the others&mdash;they're perfectly correlated. Therefore, the columns of $\\mathbf{X}$ are linearly *dependent*, which explains the error.\n",
    "\n",
    "<a id=\"cell3\"></a>\n",
    "# Making the normal equation usable again\n",
    "\n",
    "By dropping one of the one-hot encoded columns from each categorical feature, we ensure there are no \"reference\" columns&mdash;the remaining columns become linearly independent.\n",
    "\n",
    "Let's verify this works by implementing it; `get_dummies` even has a dedicated parameter `drop_first`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1_banana</th>\n",
       "      <th>var1_orange</th>\n",
       "      <th>var1_pear</th>\n",
       "      <th>var2_dog</th>\n",
       "      <th>var2_fish</th>\n",
       "      <th>bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var1_banana  var1_orange  var1_pear  var2_dog  var2_fish  bias\n",
       "0            0            0          0         1          0   1.0\n",
       "1            1            0          0         0          0   1.0\n",
       "2            0            0          1         0          1   1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode categorical features and drop first value column\n",
    "X_dropped = pd.get_dummies(training_set[['var1', 'var2']], drop_first=True)\n",
    "\n",
    "# Add bias column\n",
    "X_dropped['bias'] = np.ones(X.shape[0])\n",
    "\n",
    "# Display first three rows\n",
    "X_dropped.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `var1_apple` and `var2_cat` were dropped. Let's reattempt to use the normal equation to identify the parameters of the OLS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var1_banana    14.0\n",
       "var1_orange   -22.0\n",
       "var1_pear      63.0\n",
       "var2_dog       20.0\n",
       "var2_fish     -14.0\n",
       "bias          -10.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute parameters of OLS model\n",
    "OLS_theta = np.linalg.inv(X_dropped.T @ X_dropped) @ (X_dropped.T @ y)\n",
    "\n",
    "# Label parameters with feature names\n",
    "pd.Series(OLS_theta, index=X_dropped.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smooth sailing this time. Therefore, when using the normal equation to create an OLS model, you *must* drop one of the one-hot encoded columns from each categorical feature.\n",
    "\n",
    "<a id=\"cell4\"></a>\n",
    "# Regularizing improves predictions and then some\n",
    "\n",
    "OLS models are handy when we'd like to summarize linear trends for data we *already have*. When the goal is prediction however, these models are seldom useful because of their [numerous pitfalls](https://www.clockbackward.com/2009/06/18/ordinary-least-squares-linear-regression-flaws-problems-and-pitfalls/). In particular, OLS models tend to generalize poorly to new data (aka overfitting).\n",
    "\n",
    "To prevent overfitting, applying some form of **regularization** is a no-brainer. $\\ell_2$ regularization involves adding a penalty term&mdash;square of the $\\ell_2$ norm of $\\vec{\\theta}$&mdash;to the objective function. Applying $\\ell_2$ regularization to the OLS objective function yields\n",
    "\n",
    "$$J(\\vec{\\theta}) =  {\\left\\lVert  \\mathbf{y} - \\mathbf{X}\\vec{\\theta} \\right\\rVert_2}^2 + \\alpha{\\left\\lVert \\vec{\\theta} \\right\\rVert_2}^2$$\n",
    "\n",
    "where $\\alpha$ is a positive scalar hyperparameter that controls the degree of regularization (higher = more regularization).\n",
    "\n",
    "We need to solve a new partial differential equation $\\frac{\\partial J}{\\partial\\vec{\\theta}} = 0$; fortunately, it too has a closed-form solution\n",
    "\n",
    "$$\\vec{\\theta} = (\\mathbf{X}^T\\mathbf{X} + \\alpha \\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "where $\\mathbf{I}$ is an identity matrix with the same dimensions as the Gram matrix. Let's identify the parameters of the $\\ell_2$ regularized model using $\\alpha = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var1_banana     0.246537\n",
       "var1_orange    -7.501385\n",
       "var1_pear      32.678670\n",
       "var2_dog       -0.504155\n",
       "var2_fish     -13.049861\n",
       "bias            3.506925\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_L2_reg_model(X, y, alpha):\n",
    "    \"\"\"\n",
    "    Generate a L2 regularized linear regression model.\n",
    "    \n",
    "    This function uses the closed-form solution to compute the parameters of\n",
    "    an L2 regularized linear regression model.\n",
    "    \n",
    "    Args:\n",
    "        X (DataFrame): table containing features\n",
    "        y (Series): table containing target variable\n",
    "        alpha (float): positive scalar controlling regularization strength\n",
    "            (higher = more regularization)\n",
    "    \n",
    "    Returns:\n",
    "        theta (Series): table containing identified parameters of model\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute identity matrix \n",
    "    I = np.identity((X.T @ X).shape[0])\n",
    "\n",
    "    # Compute parameters\n",
    "    theta = np.linalg.inv(X.T @ X + alpha * I) @ (X.T @ y)\n",
    "\n",
    "    # Label parameters with feature names\n",
    "    theta = pd.Series(theta, index=X.columns)\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# Create L2 regularized model after dropping columns \n",
    "create_L2_reg_model(X_dropped, y, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regularized model will generally perform better on new data than an OLS model. In practice however, we'd tune the value of $\\alpha$ using cross-validation to maximize model performance.\n",
    "\n",
    "<a id=\"cell5\"></a>\n",
    "# Don't bother dropping columns when regularizing\n",
    "\n",
    "Having understood the benefits of regularization, let's try to generate a $\\ell_2$ regularized model with the closed-form solution but instead use the original one-hot encoded features prior to dropping any columns. We'll probably run into the singular matrix error again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var1_apple     -9.617518\n",
       "var1_banana    -4.306569\n",
       "var1_orange    -9.543066\n",
       "var1_pear      27.564964\n",
       "var2_cat        8.515328\n",
       "var2_dog        2.988321\n",
       "var2_fish      -7.405839\n",
       "bias            4.097810\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create L2 regularized model using original one-hot encoded features\n",
    "create_L2_reg_model(X, y, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, shouldn't NumPy have gotten angry? How were we still able to create a model? The answer is because in the closed-form solution of the $\\ell_2$ regularized model above, **the matrix $(\\mathbf{X}^T\\mathbf{X} + \\alpha \\mathbf{I})$ is almost surely nonsingular**. I'll prove it:\n",
    "\n",
    "- $(\\mathbf{X}^T\\mathbf{X})^T = \\mathbf{X}^T (\\mathbf{X}^T)^T = \\mathbf{X}^T \\mathbf{X}$. Therefore, $\\mathbf{X}^T\\mathbf{X}$ is a $n \\times n$ symmetric matrix with exactly $n$ eigenvalues $\\lambda_i = \\lambda_1, \\lambda_2, \\dots \\lambda_n$.\n",
    "- When $\\alpha = -\\lambda_i$, $\\det(\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I}) = 0$ and, therefore, $(\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I})$ is singular\n",
    "- When $\\alpha \\neq -\\lambda_i$, the eigenvalues of $(\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I})$ are $(\\lambda_1 + \\alpha), \\dots (\\lambda_n + \\alpha)$, all of which are nonzero and, therefore, $(\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I})$ is nonsingular\n",
    "- $(\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I})$ is nonsingular $\\forall\\{\\alpha \\in \\mathbb{R} \\mid \\alpha \\neq -\\lambda_i\\}$. Therefore, $(\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I})$ is almost surely nonsingular\n",
    "\n",
    "[\"Almost surely\"](https://math.stackexchange.com/questions/1443015/why-do-we-say-almost-surely-in-probability-theory) is an expression from probability theory describing events that occur with $P = 1$ within an infinitely large sample space. Therefore, as long as $\\alpha$ isn't the negative of an eigenvalue of $\\mathbf{X}^T\\mathbf{X}$, there exist infinitely many values of $\\alpha$ that make $(\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I})$ nonsingular. Practically any perturbation of a singular matrix makes it nonsingular!\n",
    "\n",
    "Consequently, if we apply the *tiniest* bit of regularization (whether it's $\\ell_2$, $\\ell_1$, or elastic net), we can handle features that are perfectly correlated without removing any columns. Regularization also innately addresses the [effects of multicollinearity](https://stats.stackexchange.com/a/184023)&mdash;it's pretty awesome. \n",
    "\n",
    "But if you *are* regularizing, there's no need to drop one of the one-hot encoded columns from each categorical feature&mdash;math's got your back.\n",
    "\n",
    "<a id=\"cell6\"></a>\n",
    "# Skip dropping columns when using iterative numerical methods\n",
    "\n",
    "As elegant as they are, the closed-form solutions are seldom utilized in practice. That's because matrix inversion is stupidly expensive. The time complexity of inverting an $n \\times n$ matrix is $O(n^3)$ when using Gaussian elimination; more optimized algorithms can bring it down to about $O(n^{2.4})$. Unless it has a few hundred columns (rarely the case with real world datasets), you shouldn't attempt to invert a matrix.\n",
    "\n",
    "Instead of relying on a closed-form solution, we machine learning practitioners estimate parameters via some efficient iterative numerical method such as gradient descent. Because iterative numerical methods&mdash;with or without regularization&mdash;don't involve matrix inversions, there's no reason to drop one of the one-hot encoded columns from each categorical feature when using them.\n",
    "\n",
    "<a id=\"cell7\"></a>\n",
    "# Maybe just stop dropping columns altogether\n",
    "\n",
    "So far we've discussed a few situations where removing one of the one-hot encoded columns isn't mandatory. However, dropping these columns can also have unforeseen, deleterious consequences.\n",
    "\n",
    "Did you notice that the parameters between one-hot encoded features had different values depending on whether columns were removed or not? For example, when columns are dropped $\\theta_{var1\\_banana} = -4.307$ and $\\theta_{var2\\_dog} = 2.988$; otherwise, $\\theta_{var1\\_banana} = 0.247$ and $\\theta_{var2\\_dog} = -0.504$. If we were planning to use these parameters to get a sense of feature importance, dropping columns would tell a whole another story!\n",
    "\n",
    "Because we alter the model's parameters by dropping one-hot encoded columns, we also change its predictions. What's more alarming is that dropping a different column from each categorical feature yields an entirely new set of parameters.\n",
    "\n",
    "For example, instead of `var1_apple` and `var2_cat`, let's drop `var1_banana` and `var2_dog` from the one-hot encoded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var1_apple     -8.651452\n",
       "var1_orange    -8.199170\n",
       "var1_pear      28.286307\n",
       "var2_cat        6.639004\n",
       "var2_fish      -8.294606\n",
       "bias            4.398340\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop different one-hot encoded columns from each categorical feature\n",
    "X_dropped = X.drop(['var1_banana', 'var2_dog'], axis=1)\n",
    "\n",
    "# Create L2 regularized model after dropping different set of columns\n",
    "create_L2_reg_model(X_dropped, y, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we arrive at a different model depending on the particular set of columns removed, how do we pick the right model? There's no good answer here&mdash;removing columns isn't trivial. You're better off staying objective and leaving one-hot encoded features alone.\n",
    "\n",
    "<a id=\"cell8\"></a>\n",
    "# Conclusions\n",
    "\n",
    "Feature engineering is the most important aspect of creating an effective model&mdash;you want to get it right. When dealing with categorical features, a common convention is to drop one of the one-hot encoded columns from each feature. Here we discovered this convention is *only required* when creating an OLS model with the normal equation.\n",
    "\n",
    "However, a cornerstone of machine learning is to produce a highly predictive model; therefore, we rarely turn to OLS models and *always* apply regularization. Even if we were to create a $\\ell_2$ regularized model with a closed-form solution, the [gorgeous math](https://en.wikipedia.org/wiki/Mathematical_beauty) behind regularization would lift the obligation of removing one-hot encoded columns.\n",
    "\n",
    "Nevertheless, the normal equation and other closed-form solutions are seldom practical due to their computational cost. Instead, we machine learning practitioners prefer creating linear regression models using [iterative numerical methods](http://ruder.io/optimizing-gradient-descent/) that don't demand dropping one-hot encoded columns.\n",
    "\n",
    "Finally, we found that dropping one-hot encoded columns tampers with a linear regression model's parameters and predictions. We also end up with a distinct model depending on which set of columns we happened to drop.\n",
    "\n",
    "In summary, we've uncovered one unlikely usecase where removing one of the one-hot encoded from each categorical feature is crucial for creating a linear regression model, two common situations when it's unnecessary, and two reasons why it's perilous. I'll leave it to you.\n",
    "\n",
    "What about *logistic* regression? The same reasons actually apply to generalized linear models. There's even less of a reason to drop one-hot encoded columns when using logistic regression because there is no known closed-form solution for identifying its parameters. We always rely on an iterative numerical method. That is, unless your training set has [two examples](https://www.tandfonline.com/doi/abs/10.1080/02664763.2014.932760).\n",
    "\n",
    "---\n",
    "\n",
    "**Side note:** I recommend avoiding pandas' `get_dummies` and switching to a more robust one-hot encoder, such as `OneHotEncoder` from scikit-learn&mdash;it's designed to handle these frequent scenarios:\n",
    "\n",
    "- A categorical feature containing values that appear in the test set but not the training set\n",
    "- A categorical feature in the test set containing a subset of the total possible values\n",
    "\n",
    "Notice how `OneHotEncoder` doesn't let us drop one-hot encoded columns..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
